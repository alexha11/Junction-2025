"""LLM-based explainability for optimization schedules."""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any
from datetime import datetime

import httpx

logger = logging.getLogger(__name__)

# Suppress httpx INFO level HTTP request logs (only show WARNING/ERROR)
logging.getLogger("httpx").setLevel(logging.WARNING)


@dataclass
class ScheduleMetrics:
    """Metrics for schedule explanation."""
    total_energy_kwh: float
    total_cost_eur: float
    avg_l1_m: float
    min_l1_m: float
    max_l1_m: float
    num_pumps_used: int
    avg_outflow_m3_s: float
    price_range_eur_mwh: tuple[float, float]
    risk_level: str
    optimization_mode: str


@dataclass
class OptimalWindow:
    """Identified optimal window for operation."""
    start_hour: int  # 0-23
    end_hour: int  # 0-23
    window_type: str  # CHEAP, RAINY, AVOID_SURGE, etc.
    reason: str  # Why this window is optimal
    priority: int  # 1 (highest) to 5 (lowest)
    avg_price_eur_mwh: float
    avg_inflow_m3_s: float
    confidence: float  # 0.0-1.0


@dataclass
class StrategicPlan:
    """24-hour strategic plan generated by LLM."""
    plan_type: str  # PUMP_AGGRESSIVE, PUMP_MINIMAL, MAINTAIN_BUFFER, BALANCED
    description: str  # Natural language description
    time_periods: List[tuple[int, int, str]]  # [(start_hour, end_hour, strategy), ...]
    reasoning: str  # Strategic reasoning
    recommended_weights: Optional[dict] = None  # Suggested objective weights
    optimal_windows: Optional[List[OptimalWindow]] = None  # Identified optimal windows for pumping
    forecast_confidence: Optional[str] = None  # 'high', 'medium', 'low'
    forecast_confidence: Optional[str] = None  # 'high', 'medium', 'low' - LLM's assessment


@dataclass
class EmergencyResponse:
    """Emergency response strategy when forecast errors are detected."""
    error_type: str  # 'inflow_surge', 'price_spike', 'l1_divergence', 'systematic_bias'
    severity: str  # 'low', 'medium', 'high', 'critical'
    immediate_actions: List[str]  # List of recommended actions
    reasoning: str  # Why these actions are needed
    constraint_adjustments: Optional[dict] = None  # Suggested constraint changes
    weight_adjustments: Optional[dict] = None  # Suggested weight changes


@dataclass
class ForecastQualityTracker:
    """Tracks forecast errors over time and learns patterns."""
    inflow_errors: List[float] = None  # List of percentage errors
    price_errors: List[float] = None  # List of percentage errors
    l1_errors: List[float] = None  # List of absolute errors (meters)
    error_timestamps: List[datetime] = None
    window_size: int = 50  # Track last N errors
    
    def __post_init__(self):
        """Initialize lists if None."""
        if self.inflow_errors is None:
            self.inflow_errors = []
        if self.price_errors is None:
            self.price_errors = []
        if self.l1_errors is None:
            self.l1_errors = []
        if self.error_timestamps is None:
            self.error_timestamps = []
    
    def add_error(
        self,
        inflow_error: Optional[float] = None,
        price_error: Optional[float] = None,
        l1_error: Optional[float] = None,
        timestamp: Optional[datetime] = None,
    ):
        """Add forecast errors to tracker."""
        if timestamp is None:
            timestamp = datetime.now()
        
        if inflow_error is not None:
            self.inflow_errors.append(inflow_error)
        if price_error is not None:
            self.price_errors.append(price_error)
        if l1_error is not None:
            self.l1_errors.append(l1_error)
        
        self.error_timestamps.append(timestamp)
        
        # Keep only last N errors
        if len(self.inflow_errors) > self.window_size:
            self.inflow_errors.pop(0)
        if len(self.price_errors) > self.window_size:
            self.price_errors.pop(0)
        if len(self.l1_errors) > self.window_size:
            self.l1_errors.pop(0)
        if len(self.error_timestamps) > self.window_size:
            self.error_timestamps.pop(0)
    
    def get_recent_errors(self, n: int = 10) -> Dict[str, List[float]]:
        """Get last N errors for each type."""
        return {
            'inflow': self.inflow_errors[-n:] if len(self.inflow_errors) >= n else self.inflow_errors,
            'price': self.price_errors[-n:] if len(self.price_errors) >= n else self.price_errors,
            'l1': self.l1_errors[-n:] if len(self.l1_errors) >= n else self.l1_errors,
        }
    
    def get_error_patterns(self) -> Dict[str, Any]:
        """Analyze error patterns and return summary statistics."""
        if not self.inflow_errors and not self.price_errors:
            return {
                'overall_quality': 'unknown',
                'inflow_mae': 0,
                'price_mae': 0,
                'l1_mae': 0,
                'trend': 'stable',
                'confidence': 'medium',
                'sample_size': 0,  # Always include sample_size
            }
        
        import numpy as np
        
        # Calculate MAE
        inflow_mae = np.mean([abs(e) for e in self.inflow_errors]) if self.inflow_errors else 0
        price_mae = np.mean([abs(e) for e in self.price_errors]) if self.price_errors else 0
        l1_mae = np.mean(self.l1_errors) if self.l1_errors else 0
        
        # Determine overall quality
        max_error = max(inflow_mae, price_mae)
        if max_error < 10 and l1_mae < 0.3:
            overall_quality = 'good'
        elif max_error < 25 and l1_mae < 0.5:
            overall_quality = 'fair'
        else:
            overall_quality = 'poor'
        
        # Analyze trend (if enough data)
        trend = 'stable'
        if len(self.inflow_errors) >= 10:
            recent_errors = self.inflow_errors[-10:]
            older_errors = self.inflow_errors[-20:-10] if len(self.inflow_errors) >= 20 else self.inflow_errors[:-10]
            if older_errors:
                recent_avg = np.mean([abs(e) for e in recent_errors])
                older_avg = np.mean([abs(e) for e in older_errors])
                if recent_avg > older_avg * 1.2:
                    trend = 'worsening'
                elif recent_avg < older_avg * 0.8:
                    trend = 'improving'
        
        # Assess confidence based on quality and trend
        if overall_quality == 'good' and trend in ('stable', 'improving'):
            confidence = 'high'
        elif overall_quality == 'poor' or trend == 'worsening':
            confidence = 'low'
        else:
            confidence = 'medium'
        
        return {
            'overall_quality': overall_quality,
            'inflow_mae': inflow_mae,
            'price_mae': price_mae,
            'l1_mae': l1_mae,
            'trend': trend,
            'confidence': confidence,
            'sample_size': len(self.inflow_errors),
        }
    
    def get_surge_period_confidence(self, forecast_inflow: float) -> str:
        """Assess confidence for surge periods based on historical errors.
        
        Returns 'low', 'medium', or 'high' confidence for surge predictions.
        """
        if not self.inflow_errors:
            return 'medium'
        
        import numpy as np
        
        # Check if inflow errors are higher during surge-like conditions
        # If forecast is high (>1.5x average), assess confidence
        recent_errors = self.inflow_errors[-20:] if len(self.inflow_errors) >= 20 else self.inflow_errors
        avg_error = np.mean([abs(e) for e in recent_errors])
        
        # High error rate = low confidence in forecasts
        if avg_error > 25:
            return 'low'
        elif avg_error > 15:
            return 'medium'
        else:
            return 'high'


class LLMExplainer:
    """Generate natural language explanations using Featherless LLM."""

    def __init__(
        self,
        api_base: Optional[str] = None,
        api_key: Optional[str] = None,
        model: str = "llama-3.1-8b-instruct",
    ):
        self.api_base = api_base
        self.api_key = api_key
        self.model = model

    async def generate_explanation(
        self,
        metrics: ScheduleMetrics,
        strategic_guidance: list[str],
        current_state_description: str = "",
        strategic_plan: Optional[StrategicPlan] = None,
    ) -> str:
        """Generate natural language explanation for the schedule."""
        
        # If no LLM available, return structured explanation
        if not self.api_base or not self.api_key:
            logger.debug("LLM not available: missing API credentials")
            return self._generate_fallback_explanation(metrics, strategic_guidance)
        
        try:
            # Log strategic guidance being sent to LLM (at DEBUG level)
            guidance_summary = ", ".join(set(strategic_guidance[:8]))  # Show first 8 unique
            logger.debug(f"LLM Input - Strategic Guidance: {guidance_summary}")
            logger.debug(f"LLM Input - Full Guidance List: {strategic_guidance[:8]}")  # First 8 time steps
            
            prompt = self._build_prompt(
                metrics, strategic_guidance, current_state_description, strategic_plan
            )
            logger.debug(f"LLM: Calling {self.api_base} with model {self.model}")
            logger.debug(f"LLM Prompt: {prompt[:200]}...")  # Log first 200 chars of prompt
            logger.debug(f"LLM Full Prompt:\n{prompt}")  # Full prompt at DEBUG level
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(
                    f"{self.api_base}/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {
                                "role": "system",
                                "content": "You are an expert operator assistant for wastewater pumping optimization. Provide clear, concise explanations of pump schedules in 2-3 sentences.",
                            },
                            {
                                "role": "user",
                                "content": prompt,
                            },
                        ],
                        "temperature": 0.7,
                        "max_tokens": 200,
                    },
                )
                response.raise_for_status()
                data = response.json()
                explanation = data["choices"][0]["message"]["content"].strip()
                logger.debug(f"LLM: Successfully received explanation ({len(explanation)} chars)")
                logger.debug(f"LLM Response: {explanation}")
                return explanation
        except httpx.TimeoutException as e:
            logger.error(f"LLM: Request timeout after 10s: {e}")
            return self._generate_fallback_explanation(metrics, strategic_guidance)
        except httpx.HTTPStatusError as e:
            logger.error(f"LLM: HTTP error {e.response.status_code}: {e.response.text}")
            return self._generate_fallback_explanation(metrics, strategic_guidance)
        except Exception as e:
            logger.error(f"LLM: Unexpected error: {e}", exc_info=True)
            # Fallback to rule-based explanation
            return self._generate_fallback_explanation(metrics, strategic_guidance)

    def _build_prompt(
        self,
        metrics: ScheduleMetrics,
        strategic_guidance: list[str],
        current_state_description: str,
        strategic_plan: Optional[StrategicPlan] = None,
    ) -> str:
        """Build prompt for LLM focusing on strategy explanation."""
        guidance_summary = ", ".join(set(strategic_guidance[:4]))
        
        # Count guidance types for better context
        cheap_count = sum(1 for g in strategic_guidance if g == "CHEAP")
        expensive_count = sum(1 for g in strategic_guidance if g == "EXPENSIVE")
        surge_count = sum(1 for g in strategic_guidance if g == "SURGE_RISK")
        normal_count = sum(1 for g in strategic_guidance if g == "NORMAL")
        
        # Determine strategy context
        strategy_context = []
        if cheap_count > 0:
            strategy_context.append(f"electricity prices are low in {cheap_count} periods")
        if expensive_count > 0:
            strategy_context.append(f"electricity prices are high in {expensive_count} periods")
        if surge_count > 0:
            strategy_context.append(f"inflow surge risk in {surge_count} periods")
        if metrics.max_l1_m > 7.0:
            strategy_context.append("tunnel level is approaching maximum")
        if metrics.min_l1_m < 1.0:
            strategy_context.append("tunnel level is near minimum")
        
        strategy_context_str = ", ".join(strategy_context) if strategy_context else "normal operating conditions"
        
        # Log guidance statistics (at DEBUG level)
        logger.debug(f"LLM Input - Guidance Statistics: CHEAP={cheap_count}, EXPENSIVE={expensive_count}, SURGE_RISK={surge_count}, NORMAL={normal_count}")
        
        # Include strategic plan if available
        strategic_plan_context = ""
        if strategic_plan:
            strategic_plan_context = f"""

24-Hour Strategic Plan: {strategic_plan.plan_type}
Strategic Overview: {strategic_plan.description}
Strategic Reasoning: {strategic_plan.reasoning}

This schedule implements the tactical actions aligned with the above strategic plan."""
        
        prompt = f"""You are an expert wastewater pumping system operator. Explain the STRATEGY behind this pump schedule decision.

{current_state_description}
{strategic_plan_context}

Strategic Context: {guidance_summary} - This indicates that {strategy_context_str}.

Schedule Outcomes:
- Energy consumption: {metrics.total_energy_kwh:.1f} kWh
- Cost: {metrics.total_cost_eur:.2f} EUR
- Tunnel level trajectory: {metrics.min_l1_m:.2f} - {metrics.max_l1_m:.2f} m (safe range: 0.5-8.0 m)
- Pumps utilized: {metrics.num_pumps_used} pumps
- Electricity price range: {metrics.price_range_eur_mwh[0]:.1f} - {metrics.price_range_eur_mwh[1]:.1f} EUR/MWh

Explain the STRATEGY in 2-3 sentences:
1. Why was this specific strategy chosen given the current system state and price conditions?
2. What trade-offs are being made (cost vs safety, energy vs outflow)?
3. How does the strategy optimize for the current conditions (cheap/expensive prices, tunnel level, inflow)?

Focus on the strategic reasoning and decision logic, not just what the schedule does."""
        
        return prompt

    async def generate_strategic_plan(
        self,
        forecast_24h_timestamps: List[datetime],
        forecast_24h_inflow: List[float],
        forecast_24h_price: List[float],
        current_l1_m: float,
        l1_min_m: float,
        l1_max_m: float,
        forecast_quality_tracker: Optional[ForecastQualityTracker] = None,
    ) -> Optional[StrategicPlan]:
        """Generate 24-hour strategic plan using LLM analysis.
        
        Args:
            forecast_24h_timestamps: List of timestamps for 24h forecast
            forecast_24h_inflow: Inflow forecast for each time step (m³/s)
            forecast_24h_price: Price forecast for each time step (EUR/MWh)
            current_l1_m: Current tunnel level (m)
            l1_min_m: Minimum allowed tunnel level (m)
            l1_max_m: Maximum allowed tunnel level (m)
        
        Returns:
            StrategicPlan or None if LLM unavailable
        """
        # If no LLM available, return None (will fall back to algorithmic)
        if not self.api_base or not self.api_key:
            logger.debug("LLM not available for strategic planning")
            return None
        
        try:
            prompt = self._build_strategic_plan_prompt(
                forecast_24h_timestamps,
                forecast_24h_inflow,
                forecast_24h_price,
                current_l1_m,
                l1_min_m,
                l1_max_m,
                forecast_quality_tracker=forecast_quality_tracker,
            )
            
            logger.debug("LLM: Generating 24-hour strategic plan...")
            logger.debug(f"Strategic Plan Prompt:\n{prompt}")
            
            async with httpx.AsyncClient(timeout=15.0) as client:
                response = await client.post(
                    f"{self.api_base}/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {
                                "role": "system",
                                "content": "You are an expert wastewater pumping system strategist. Analyze 24-hour forecasts and generate a high-level strategic plan with time periods and recommended approach. Respond in a structured format.",
                            },
                            {
                                "role": "user",
                                "content": prompt,
                            },
                        ],
                        "temperature": 0.5,  # Lower temperature for more consistent strategic planning
                        "max_tokens": 400,
                    },
                )
                response.raise_for_status()
                data = response.json()
                plan_text = data["choices"][0]["message"]["content"].strip()
                
                logger.debug(f"LLM: Successfully received strategic plan ({len(plan_text)} chars)")
                logger.debug(f"Strategic Plan Response: {plan_text}")
                
                # Parse the strategic plan from LLM response
                parsed_plan = self._parse_strategic_plan(plan_text, forecast_24h_timestamps)
                
                # If forecast_confidence not set by LLM, use quality tracker's confidence
                if parsed_plan and not parsed_plan.forecast_confidence and forecast_quality_tracker:
                    quality_patterns = forecast_quality_tracker.get_error_patterns()
                    parsed_plan.forecast_confidence = quality_patterns.get('confidence', 'medium')
                
                return parsed_plan
                
        except Exception as e:
            logger.warning(f"LLM: Failed to generate strategic plan: {e}")
            return None
    
    async def generate_emergency_response(
        self,
        error_type: str,
        error_magnitude: float,
        forecast_value: float,
        actual_value: float,
        current_l1_m: float,
        l1_min_m: float,
        l1_max_m: float,
        predicted_l1_m: Optional[float] = None,
    ) -> Optional[EmergencyResponse]:
        """Generate emergency response strategy when forecast errors are detected.
        
        Args:
            error_type: Type of error ('inflow_surge', 'price_spike', 'l1_divergence', 'systematic_bias')
            error_magnitude: Magnitude of error (percentage or absolute value)
            forecast_value: The forecasted value
            actual_value: The actual observed value
            current_l1_m: Current tunnel level
            l1_min_m: Minimum allowed tunnel level
            l1_max_m: Maximum allowed tunnel level
            predicted_l1_m: Predicted L1 from optimization (if available)
        
        Returns:
            EmergencyResponse or None if LLM unavailable
        """
        if not self.api_base or not self.api_key:
            logger.debug("LLM not available for emergency response")
            return None
        
        try:
            # Determine severity
            if error_type == 'inflow_surge':
                if error_magnitude > 100:
                    severity = 'critical'
                elif error_magnitude > 50:
                    severity = 'high'
                elif error_magnitude > 30:
                    severity = 'medium'
                else:
                    severity = 'low'
            elif error_type == 'price_spike':
                if error_magnitude > 100:
                    severity = 'high'
                elif error_magnitude > 50:
                    severity = 'medium'
                else:
                    severity = 'low'
            elif error_type == 'l1_divergence':
                if error_magnitude > 1.0:  # meters
                    severity = 'critical'
                elif error_magnitude > 0.5:
                    severity = 'high'
                else:
                    severity = 'medium'
            else:
                severity = 'medium'
            
            prompt = self._build_emergency_response_prompt(
                error_type, error_magnitude, forecast_value, actual_value,
                current_l1_m, l1_min_m, l1_max_m, predicted_l1_m, severity
            )
            
            logger.info(f"LLM: Generating emergency response for {error_type} (severity: {severity})")
            logger.debug(f"Emergency Response Prompt:\n{prompt}")
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(
                    f"{self.api_base}/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {
                                "role": "system",
                                "content": "You are an expert wastewater pumping system operator responding to forecast errors. Provide immediate, actionable emergency response strategies. Be specific about constraint adjustments and pumping actions.",
                            },
                            {
                                "role": "user",
                                "content": prompt,
                            },
                        ],
                        "temperature": 0.3,  # Low temperature for consistent emergency responses
                        "max_tokens": 300,
                    },
                )
                response.raise_for_status()
                data = response.json()
                response_text = data["choices"][0]["message"]["content"].strip()
                
                logger.debug(f"LLM Emergency Response: {response_text}")
                
                # Parse the emergency response
                return self._parse_emergency_response(
                    response_text, error_type, severity, error_magnitude
                )
                
        except Exception as e:
            logger.warning(f"LLM: Failed to generate emergency response: {e}")
            return None
    
    def _build_emergency_response_prompt(
        self,
        error_type: str,
        error_magnitude: float,
        forecast_value: float,
        actual_value: float,
        current_l1_m: float,
        l1_min_m: float,
        l1_max_m: float,
        predicted_l1_m: Optional[float],
        severity: str,
    ) -> str:
        """Build prompt for emergency response generation."""
        
        error_description = {
            'inflow_surge': f"Inflow surge detected: Forecast was {forecast_value:.2f} m³/s but actual is {actual_value:.2f} m³/s (error: {error_magnitude:.1f}%)",
            'price_spike': f"Price spike detected: Forecast was {forecast_value:.1f} EUR/MWh but actual is {actual_value:.1f} EUR/MWh (error: {error_magnitude:.1f}%)",
            'l1_divergence': f"L1 divergence detected: Predicted {predicted_l1_m:.2f}m but actual is {current_l1_m:.2f}m (error: {error_magnitude:.2f}m)",
            'systematic_bias': f"Systematic forecast bias detected: Consistent {error_magnitude:.1f}% error over multiple steps",
        }.get(error_type, f"Forecast error detected: {error_type}")
        
        prompt = f"""EMERGENCY SITUATION - Forecast Error Detected

{error_description}

Current System State:
- Tunnel Level (L1): {current_l1_m:.2f} m
- L1 Safe Range: {l1_min_m:.1f} - {l1_max_m:.1f} m
- Severity: {severity.upper()}

Provide an emergency response strategy with:
1. IMMEDIATE_ACTIONS: List 2-3 specific actions to take right now
2. REASONING: Explain why these actions are necessary
3. CONSTRAINT_ADJUSTMENTS: Suggest L1_min and L1_max adjustments (if needed)
4. WEIGHT_ADJUSTMENTS: Suggest optimization weight changes (safety_margin, cost, energy)

Format your response clearly with these sections."""
        
        return prompt
    
    def _parse_emergency_response(
        self,
        response_text: str,
        error_type: str,
        severity: str,
        error_magnitude: float,
    ) -> EmergencyResponse:
        """Parse LLM response into EmergencyResponse object."""
        # Extract immediate actions
        immediate_actions = []
        reasoning = ""
        constraint_adjustments = None
        weight_adjustments = None
        
        # Simple parsing - look for key sections
        lines = response_text.split('\n')
        current_section = None
        
        for line in lines:
            line_lower = line.lower().strip()
            if 'immediate' in line_lower or 'actions' in line_lower:
                current_section = 'actions'
            elif 'reasoning' in line_lower or 'why' in line_lower:
                current_section = 'reasoning'
            elif 'constraint' in line_lower:
                current_section = 'constraints'
            elif 'weight' in line_lower:
                current_section = 'weights'
            elif line.strip() and not line.strip().startswith('#'):
                if current_section == 'actions':
                    if line.strip().startswith('-') or line.strip()[0].isdigit():
                        immediate_actions.append(line.strip().lstrip('- ').lstrip('123456789. '))
                elif current_section == 'reasoning':
                    reasoning += line.strip() + " "
        
        # If parsing failed, use the full response as reasoning
        if not immediate_actions and not reasoning:
            reasoning = response_text
            # Try to extract actions from numbered/bulleted lists
            for line in lines:
                if line.strip() and (line.strip().startswith('-') or line.strip()[0].isdigit()):
                    immediate_actions.append(line.strip().lstrip('- ').lstrip('123456789. '))
        
        # Default actions if none found
        if not immediate_actions:
            if error_type == 'inflow_surge':
                immediate_actions = [
                    "Activate all available pumps to handle surge",
                    "Reduce L1_max safety margin by 1.0-1.5m",
                    "Monitor L1 every 5 minutes"
                ]
            elif error_type == 'price_spike':
                immediate_actions = [
                    "Reduce pumping to minimum required for safety",
                    "Prioritize safety over cost optimization",
                    "Plan to pump more during next cheap period"
                ]
            elif error_type == 'l1_divergence':
                immediate_actions = [
                    "Immediately re-optimize with actual L1 value",
                    "Increase safety margin weight",
                    "Adjust pumping to correct L1 trajectory"
                ]
            else:
                immediate_actions = [
                    "Re-optimize with corrected forecast values",
                    "Apply conservative safety margins",
                    "Increase monitoring frequency"
                ]
        
        return EmergencyResponse(
            error_type=error_type,
            severity=severity,
            immediate_actions=immediate_actions,
            reasoning=reasoning.strip() or f"Emergency response for {error_type} with {severity} severity",
            constraint_adjustments=constraint_adjustments,
            weight_adjustments=weight_adjustments,
        )
    
    def _build_strategic_plan_prompt(
        self,
        timestamps: List[datetime],
        inflows: List[float],
        prices: List[float],
        current_l1: float,
        l1_min: float,
        l1_max: float,
        forecast_quality_tracker: Optional[ForecastQualityTracker] = None,
    ) -> str:
        """Build prompt for LLM strategic planning."""
        # Calculate statistics
        avg_price = sum(prices) / len(prices) if prices else 0.0
        min_price = min(prices) if prices else 0.0
        max_price = max(prices) if prices else 0.0
        avg_inflow = sum(inflows) / len(inflows) if inflows else 0.0
        max_inflow = max(inflows) if inflows else 0.0
        
        # Identify cheap/expensive periods
        price_std = (sum((p - avg_price) ** 2 for p in prices) / len(prices)) ** 0.5 if prices else 1.0
        
        # Build time periods summary (group by 6-hour windows)
        period_summaries = []
        for i in range(0, min(24, len(prices)), 6):
            end_i = min(i + 6, len(prices))
            period_prices = prices[i:end_i]
            period_inflows = inflows[i:end_i]
            period_avg_price = sum(period_prices) / len(period_prices)
            period_max_inflow = max(period_inflows) if period_inflows else 0.0
            
            price_category = "CHEAP" if period_avg_price < avg_price - 0.3 * price_std else \
                            "EXPENSIVE" if period_avg_price > avg_price + 0.3 * price_std else "NORMAL"
            surge_risk = "HIGH" if period_max_inflow > avg_inflow * 1.3 else "LOW"
            
            hour_start = i
            hour_end = end_i
            period_summaries.append(
                f"Hours {hour_start}-{hour_end}: Price={period_avg_price:.1f} EUR/MWh ({price_category}), "
                f"Max inflow={period_max_inflow:.2f} m³/s (Surge risk: {surge_risk})"
            )
        
        # L1 status
        l1_range = l1_max - l1_min
        l1_normalized = (current_l1 - l1_min) / l1_range if l1_range > 0 else 0.5
        l1_status = "LOW" if l1_normalized < 0.3 else "HIGH" if l1_normalized > 0.7 else "NORMAL"
        
        # Include forecast quality information if available
        forecast_quality_section = ""
        if forecast_quality_tracker:
            quality_patterns = forecast_quality_tracker.get_error_patterns()
            quality_summary = f"""
FORECAST QUALITY ASSESSMENT (Based on Historical Errors):
- Overall Quality: {quality_patterns.get('overall_quality', 'unknown').upper()}
- Inflow Forecast Error (MAE): {quality_patterns.get('inflow_mae', 0):.1f}%
- Price Forecast Error (MAE): {quality_patterns.get('price_mae', 0):.1f}%
- L1 Prediction Error (MAE): {quality_patterns.get('l1_mae', 0):.2f} m
- Trend: {quality_patterns.get('trend', 'stable').upper()}
- Forecast Confidence: {quality_patterns.get('confidence', 'medium').upper()}
- Sample Size: {quality_patterns.get('sample_size', 0)} recent observations

IMPORTANT: When forecast confidence is LOW, recommend PUMP_CONSERVATIVE strategy.
When forecast confidence is LOW and surge risk is HIGH, suggest building larger buffer before surge periods.
"""
            
            # Assess confidence for surge periods
            max_forecast_inflow = max(inflows) if inflows else 0
            avg_forecast_inflow = sum(inflows) / len(inflows) if inflows else 0
            surge_periods = []
            for i, inflow in enumerate(inflows):
                if inflow > avg_forecast_inflow * 1.3:  # Surge risk
                    hour = i // 4  # Assuming 15-min steps, 4 per hour
                    surge_confidence = forecast_quality_tracker.get_surge_period_confidence(inflow)
                    surge_periods.append(f"  Hour {hour}: Forecast={inflow:.2f} m³/s, Confidence={surge_confidence.upper()}")
            
            if surge_periods:
                forecast_quality_section += f"\nSURGE PERIOD ASSESSMENT:\n" + "\n".join(surge_periods) + "\n"
        else:
            forecast_quality_section = "\nForecast Quality: No historical error data available (assume MEDIUM confidence).\n"
        
        prompt = f"""Analyze this 24-hour wastewater pumping forecast and create a strategic plan.

Current System State:
- Tunnel level L1: {current_l1:.2f} m (Status: {l1_status}, Range: {l1_min:.1f}-{l1_max:.1f} m)
- Price statistics: Avg={avg_price:.1f} EUR/MWh, Min={min_price:.1f}, Max={max_price:.1f} EUR/MWh
- Inflow statistics: Avg={avg_inflow:.2f} m³/s, Max={max_inflow:.2f} m³/s

{forecast_quality_section}

24-Hour Forecast Summary (6-hour periods):
{chr(10).join(period_summaries)}

Generate a strategic plan with the following format:
PLAN_TYPE: [PUMP_AGGRESSIVE | PUMP_MINIMAL | MAINTAIN_BUFFER | PUMP_CONSERVATIVE | BALANCED]
DESCRIPTION: [2-3 sentence strategic overview]

TIME_PERIODS:
- Hours 0-6: [strategy type] - [brief reason]
- Hours 6-12: [strategy type] - [brief reason]
- Hours 12-18: [strategy type] - [brief reason]
- Hours 18-24: [strategy type] - [brief reason]

REASONING: [Explain the overall strategic approach, trade-offs, and key decision factors]

RECOMMENDED_APPROACH: [How should the 2-hour tactical optimizer adjust its weights/objectives based on this strategy?]

Focus on:
1. When to pump aggressively (exploit cheap prices, reduce level before expensive periods) - ONLY if forecast confidence is HIGH
2. When to minimize pumping (expensive prices, maintain buffer)
3. When to build buffer (before surge risks, prepare for expensive periods)
4. When to use PUMP_CONSERVATIVE (forecast confidence is LOW, uncertainty is high)
5. Building larger buffer before surge periods when forecast confidence for surges is LOW
6. Balancing cost optimization with safety (L1 bounds) - prioritize safety when forecasts are unreliable

IMPORTANT GUIDELINES:
- If forecast confidence is LOW: Recommend PUMP_CONSERVATIVE strategy with larger safety margins
- If surge risk is HIGH and surge forecast confidence is LOW: Build larger buffer before surge period (pump more aggressively before)
- Analyze forecast error patterns and adjust strategy accordingly (if errors are worsening, be more conservative)"""
        
        return prompt
    
    def _parse_strategic_plan(self, plan_text: str, timestamps: List[datetime]) -> Optional[StrategicPlan]:
        """Parse LLM response into StrategicPlan object."""
        try:
            lines = plan_text.strip().split('\n')
            
            # Extract plan type
            plan_type = "BALANCED"
            description = ""
            reasoning = ""
            time_periods = []
            recommended_weights = None
            forecast_confidence = None
            
            current_section = None
            description_lines = []
            reasoning_lines = []
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # Parse plan type
                if line.startswith("PLAN_TYPE:"):
                    plan_type = line.split(":", 1)[1].strip().upper()
                    if plan_type not in ["PUMP_AGGRESSIVE", "PUMP_MINIMAL", "MAINTAIN_BUFFER", "PUMP_CONSERVATIVE", "BALANCED"]:
                        plan_type = "BALANCED"
                
                # Parse forecast confidence if provided
                elif line.lower().startswith("forecast_confidence:") or "confidence:" in line.lower():
                    conf_str = line.split(":", 1)[1].strip().lower()
                    if "low" in conf_str:
                        forecast_confidence = "low"
                    elif "high" in conf_str:
                        forecast_confidence = "high"
                    elif "medium" in conf_str:
                        forecast_confidence = "medium"
                
                # Parse description
                elif line.startswith("DESCRIPTION:"):
                    current_section = "description"
                    description_lines.append(line.split(":", 1)[1].strip())
                
                # Parse time periods
                elif line.startswith("TIME_PERIODS:"):
                    current_section = "time_periods"
                elif current_section == "time_periods" and line.startswith("- Hours"):
                    # Parse: "Hours 0-6: PUMP_AGGRESSIVE - reason"
                    parts = line.replace("- Hours", "").strip().split(":", 1)
                    if len(parts) == 2:
                        hour_range = parts[0].strip()
                        strategy_part = parts[1].strip().split("-", 1)
                        strategy = strategy_part[0].strip().upper()
                        reason = strategy_part[1].strip() if len(strategy_part) > 1 else ""
                        
                        # Parse hour range
                        if "-" in hour_range:
                            start_hour, end_hour = map(int, hour_range.split("-"))
                            time_periods.append((start_hour, end_hour, strategy))
                
                # Parse reasoning
                elif line.startswith("REASONING:"):
                    current_section = "reasoning"
                    reasoning_lines.append(line.split(":", 1)[1].strip())
                
                # Parse recommended approach
                elif line.startswith("RECOMMENDED_APPROACH:"):
                    current_section = "recommended"
                    # Could extract weight suggestions here if needed
                
                # Continue current section
                elif current_section == "description" and line:
                    description_lines.append(line)
                elif current_section == "reasoning" and line:
                    reasoning_lines.append(line)
            
            description = " ".join(description_lines) if description_lines else ""
            reasoning = " ".join(reasoning_lines) if reasoning_lines else ""
            
            # If no time periods parsed, create default based on plan type
            if not time_periods:
                time_periods = [(0, 24, plan_type)]
            
            # If forecast_confidence not extracted from LLM response, use quality tracker if available
            # (This will be set by caller if needed)
            
            return StrategicPlan(
                plan_type=plan_type,
                description=description or f"{plan_type} strategy for 24-hour horizon",
                time_periods=time_periods,
                reasoning=reasoning or "Strategic plan based on 24-hour forecast analysis",
                recommended_weights=None,
                forecast_confidence=forecast_confidence,
            )
            
        except Exception as e:
            logger.warning(f"Failed to parse strategic plan: {e}")
            logger.debug(f"Plan text: {plan_text[:500]}")
            return None

    def _generate_fallback_explanation(
        self,
        metrics: ScheduleMetrics,
        strategic_guidance: list[str],
    ) -> str:
        """Generate explanation without LLM."""
        parts = []
        
        # Risk-based message
        if metrics.risk_level == "critical":
            parts.append("Critical risk detected: prioritizing safety over cost.")
        elif metrics.risk_level == "high":
            parts.append("Elevated risk: balancing safety and efficiency.")
        elif metrics.risk_level == "low":
            parts.append("Low risk conditions: optimizing for cost efficiency.")
        else:
            parts.append("Normal operations: balanced optimization.")
        
        # Price-based message
        price_avg = (metrics.price_range_eur_mwh[0] + metrics.price_range_eur_mwh[1]) / 2
        if price_avg < 50:
            parts.append("Low electricity prices: increased pumping to reduce level.")
        elif price_avg > 100:
            parts.append("High electricity prices: minimal pumping while maintaining safety.")
        
        # Level-based message
        if metrics.max_l1_m > 7.0:
            parts.append("Tunnel level approaching upper bound: active pumping to prevent overflow.")
        elif metrics.min_l1_m < 1.0:
            parts.append("Tunnel level low: reduced pumping to maintain minimum level.")
        
        # Strategy message
        guidance_set = set(strategic_guidance[:4])
        if "CHEAP" in guidance_set:
            parts.append("Exploiting cheap price periods for cost savings.")
        if "EXPENSIVE" in guidance_set:
            parts.append("Minimizing pumping during expensive periods.")
        
        explanation = " ".join(parts)
        if not explanation:
            explanation = f"Optimized schedule using {metrics.optimization_mode} mode. Estimated cost: {metrics.total_cost_eur:.2f} EUR."
        
        return explanation

